# This manifest defines a SparkApplication resource for the Kubernetes Operator.
# This version is configured to run the test suite on the cluster.
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  # The name of your application instance in Kubernetes.
  name: scala-spark-gpu-tests
  # The namespace where the Spark Operator is running.
  namespace: default
spec:
  # The type of application (Scala, Python, R, or Java).
  type: Scala
  # The mode can be 'cluster' or 'client'. 'cluster' is standard for production.
  mode: cluster
  # The Spark version. MUST match the version in the Docker image and build.sbt.
  sparkVersion: "3.5.3"
  # The main class that serves as the entry point for your test runner.
  mainClass: com.example.spark.runner.TestRunner
  # The location of your application JAR inside the Docker container.
  # The 'local://' scheme is crucial to tell Spark the file is already in the image.
  mainApplicationFile: "local:///opt/spark/jars/spark-template.jar"

  # IMPORTANT: Replace this with the actual name of your Docker image after
  # building and pushing it to a registry (like Docker Hub, GCR, ECR, etc.).
  image: "localhost/spark-template:latest" # Using the image name directly

  # Explicitly set the pull policy to 'Never'.
  # This is crucial for local Kind development to ensure Kubernetes does not
  # attempt to pull the image from a remote registry and instead uses the
  # image already loaded into the Kind cluster.
  imagePullPolicy: "Never"

  # Restart policy for the Spark driver pod. For tests, we might not want to restart.
  # 'Never' ensures that a failed test run is reported as a failure immediately.
  restartPolicy:
    type: Never
    # onFailureRetries: 3
    # onFailureRetryInterval: 10
    # onSubmissionFailureRetries: 5
    # onSubmissionFailureRetryInterval: 20

  # Configuration for the Spark driver pod.
  driver:
    cores: 1
    coreLimit: "1000m" # Added coreLimit for the driver
    memory: "1024m" # e.g., 1 gigabyte
    labels:
      version: 3.5.3
      app-role: test-driver
    # A service account with permissions to create and manage executor pods is required.
    serviceAccount: spark

  # Configuration for the Spark executor pods.
  executor:
    cores: 1
    coreLimit: "1000m"
    instances: 1
    memory: "2G"
    gpu:
      name: "nvidia.com/gpu"
      quantity: 1
    labels:
      version: 3.5.3
      app-role: test-executor
    # Node affinity to schedule executors on nodes with GPU capabilities,
    # based on NFD labels.
    nodeSelector:
      "nvidia.com/gpu.present": "true" # A common label applied by NVIDIA GPU operator/NFD

  # Add Spark configuration properties.
  sparkConf:
    # Enabling RAPIDS plugin
    spark.plugins: "com.nvidia.spark.SQLPlugin"
    spark.rapids.sql.enabled: "true"
    spark.rapids.force.caller.classloader: "false" # Added new RAPIDS config
    spark.executor.resource.gpu.vendor: "nvidia.com"
    spark.executor.resource.gpu.discoveryScript: "/opt/spark/scripts/getGpusResources.sh"

    # GPU allocation and discovery settings
    spark.task.resource.gpu.amount: "1"
    spark.executor.resource.gpu.amount: "1"
    spark.kubernetes.driver.resource.gpu.vendor: "nvidia.com" # Kept for driver GPU
    spark.kubernetes.driver.resource.gpu.amount: "1" # Kept for driver GPU
    spark.kubernetes.driver.resource.gpu.discoveryScript: "/opt/spark/scripts/getGpusResources.sh" # Kept for driver GPU
    spark.kubernetes.executor.resource.gpu.vendor: "nvidia.com"
    spark.kubernetes.executor.resource.gpu.discoveryScript: "/opt/spark/scripts/getGpusResources.sh" # Updated discovery script path
    spark.rapids.shims-provider-override: "com.nvidia.spark.rapids.shims.spark355.SparkShimServiceProvider" # Added new RAPIDS shim config

    # Ephemeral storage configuration (these are correctly handled by Spark Operator
    # through these sparkConf properties, not direct pod.spec.resources)
    spark.kubernetes.driver.request.ephemeralStorage: "1Gi"
    spark.kubernetes.driver.limit.ephemeralStorage: "20Gi"
    spark.kubernetes.executor.request.ephemeralStorage: "1Gi"
    spark.kubernetes.executor.limit.ephemeralStorage: "20Gi"

    # Optional: Log detailed RAPIDS explanations for debugging and verification
    spark.rapids.sql.explain: "true"
    # Optional: For more verbose logging of RAPIDS activity
    spark.rapids.sql.logLevel: "INFO" # or DEBUG
